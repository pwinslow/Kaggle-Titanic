{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook describes my solution for the Kaggle Titanic Challenge   \n",
    "\n",
    "First, start with the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import division from future\n",
    "from __future__ import division\n",
    "\n",
    "# Basic analysis imports\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "from pandas.tools import plotting\n",
    "import numpy as np\n",
    "\n",
    "# Scikitlearn imports\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Standard visualization analysis imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in and combine training and test datasets so that all feature engineering is done to both equally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "# Extract and remove target data from training set\n",
    "target = train.Survived\n",
    "train.drop('Survived', axis=1, inplace=True)\n",
    "# Combine training and testing data\n",
    "combined_df = train.append(test)\n",
    "combined_df.reset_index(inplace=True)\n",
    "combined_df.drop('index', axis=1, inplace=True)\n",
    "# Take a quick look\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many missing values in the Age and Cabin columns as well as one in the Embarked column. To ameliorate this, when processing the raw dataset to form meaningful features for machine learning I fill in the missing values with the corresponding medians as below. Also, I form dummy variables for all categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_features():\n",
    "    \n",
    "    # Load combined dataframe\n",
    "    global combined_df\n",
    "    \n",
    "    '''\n",
    "    Each name starts with a title of some kind. I'll create a dummy variable for each separate title. \n",
    "    Since some titles are very similar to others, I'll create a reduced list to map to.\n",
    "    '''\n",
    "    title_list = list(np.unique([name.split(', ')[1].split('.')[0] for name in combined_df.Name]))\n",
    "    title_map = {\n",
    "                        'Capt': 'Military',\n",
    "                        'Col': 'Military',\n",
    "                        'Major': 'Military',\n",
    "                        'Don': 'Royalty',\n",
    "                        'Dona': 'Royalty',\n",
    "                        'Jonkheer': 'Royalty',\n",
    "                        'the Countess': 'Royalty',\n",
    "                        'Sir': 'Royalty',\n",
    "                        'Lady': 'Royalty',\n",
    "                        'Master': 'Royalty',\n",
    "                        'Dr': 'Academic',\n",
    "                        'Rev': 'Academic',\n",
    "                        'Mlle': 'Miss',\n",
    "                        'Miss': 'Miss',\n",
    "                        'Mme': 'Mrs',\n",
    "                        'Mrs': 'Mrs',\n",
    "                        'Ms': 'Mrs',\n",
    "                        'Mr': 'Mr'\n",
    "                    }\n",
    "    # Create a Title column\n",
    "    combined_df['Title'] = combined_df.Name.map(lambda x: x.split(', ')[1].split('.')[0])\n",
    "    # Map to reduced list of titles\n",
    "    combined_df.Title = combined_df.Title.map(title_map)\n",
    "    # Create dummy columns for titles. ***Keep the dummy title for the time being, we'll drop in a second***\n",
    "    title_dummies = pd.get_dummies(combined_df.Title, prefix='Title')\n",
    "    combined_df = pd.concat([combined_df, title_dummies], axis=1)\n",
    "    # Drop Name column\n",
    "    combined_df.drop('Name', axis=1, inplace=True)\n",
    "    \n",
    "    '''\n",
    "    I fill in the missing age values based on the median value organized by Sex, Class, and Title.\n",
    "    '''\n",
    "    # Fill in missing age values\n",
    "    age_groups = combined_df.groupby(['Sex', 'Pclass', 'Title'])['Age'].median()\n",
    "    # Define a function to map to correct median age based on sex, class, and title\n",
    "    def process_age(row):\n",
    "        if np.isnan(row.Age):\n",
    "            age = age_groups[row.Sex, row.Pclass, row.Title]\n",
    "        else:\n",
    "            age = row.Age \n",
    "        return age\n",
    "    combined_df.Age = combined_df.apply(process_age, axis=1)\n",
    "    \n",
    "    '''\n",
    "    I fill in the missing fare values based on the median value organized by Sex, Class, and Title.\n",
    "    '''\n",
    "    # Fill in missing fare values\n",
    "    fare_groups = combined_df.groupby(['Sex', 'Pclass', 'Title'])['Fare'].median()\n",
    "    def process_fare(row):\n",
    "        if np.isnan(row.Fare):\n",
    "            fare = fare_groups[row.Sex, row.Pclass, row.Title]\n",
    "        else:\n",
    "            fare = row.Fare \n",
    "        return fare\n",
    "    combined_df.Fare = combined_df.apply(process_fare, axis=1)\n",
    "    \n",
    "    '''\n",
    "    Since the vast majority of people embarked from Southampton, I'll just assume all missing values are 'S'\n",
    "    '''\n",
    "    combined_df.Embarked.fillna('S', inplace=True)\n",
    "    embarked_dummies = pd.get_dummies(combined_df.Embarked, prefix='Embarked')\n",
    "    combined_df = pd.concat([combined_df, embarked_dummies], axis=1)\n",
    "    combined_df.drop('Embarked', axis=1, inplace=True)\n",
    "    \n",
    "    '''\n",
    "    I define dummy variables based on which deck they were on (which is the first letter of their cabin number).\n",
    "    Also, if the cabin number is missing, then I fill it with U for unknown.\n",
    "    '''\n",
    "    # Fill missing cabin values with 'U' for unknown\n",
    "    combined_df.Cabin.fillna('U', inplace=True)\n",
    "    # Extract first letter of cabin values to act as the Deck the passenger was assigned to\n",
    "    combined_df['Deck'] = combined_df.Cabin.map(lambda x: x[0])\n",
    "    # Use dummy encoding\n",
    "    deck_dummies = pd.get_dummies(combined_df.Deck, prefix='Deck')\n",
    "    combined_df = pd.concat([combined_df, deck_dummies], axis=1)\n",
    "    combined_df.drop('Cabin', axis=1, inplace=True)\n",
    "    \n",
    "    '''\n",
    "    Process Sex feature\n",
    "    '''\n",
    "    combined_df.Sex = combined_df.Sex.map({'male':1, 'female':0})\n",
    "    \n",
    "    '''\n",
    "    Create dummy features for Class\n",
    "    '''\n",
    "    class_dummies = pd.get_dummies(combined_df.Pclass, prefix='Pclass')\n",
    "    combined_df = pd.concat([combined_df, class_dummies], axis=1)\n",
    "    \n",
    "    '''\n",
    "    The prefix for the ticket number can be another identifying feature so we'll create a set of dummy variables\n",
    "    for it.\n",
    "    '''\n",
    "    def clean_ticket(ticket):\n",
    "        ticket = ticket.replace('/', '').replace('.', '').split()\n",
    "        ticket = [x.strip() for x in ticket]\n",
    "        ticket = filter(lambda x: not x.isdigit(), ticket)\n",
    "        if len(ticket) > 0:\n",
    "            return ticket[0]\n",
    "        else:\n",
    "            return 'XXX'\n",
    "\n",
    "    combined_df['Ticket'] = combined_df.Ticket.map(clean_ticket)\n",
    "    ticket_dummies = pd.get_dummies(combined_df.Ticket, prefix='Ticket')\n",
    "    combined_df = pd.concat([combined_df, ticket_dummies], axis=1)\n",
    "    combined_df.drop('Ticket', axis=1, inplace=True)\n",
    "    \n",
    "    '''\n",
    "    We can also create a family feature which can be translated into 3 separate dummy variables for single, \n",
    "    small, and large family.\n",
    "    '''\n",
    "    combined_df['Family'] = combined_df.SibSp + combined_df.Parch\n",
    "    def family_map(size):\n",
    "        if size == 1:\n",
    "            return 'Single'\n",
    "        elif (size > 1) and (size <= 4):\n",
    "            return 'Small'\n",
    "        else:\n",
    "            return 'Large'\n",
    "    combined_df.Family = combined_df.Family.map(family_map)\n",
    "    family_dummies = pd.get_dummies(combined_df.Family, prefix='Family')\n",
    "    combined_df = pd.concat([combined_df, family_dummies], axis=1)\n",
    "    \n",
    "    # Break up the dataset again into a training and testing set\n",
    "    train_df = combined_df.loc[ 0:(train.shape[0]-1) ]\n",
    "    test_df = combined_df.loc[ train.shape[0]:(train.shape[0] + test.shape[0]) ]\n",
    "    \n",
    "    return train_df, test_df\n",
    "    \n",
    "# Process all features\n",
    "train_df, test_df = process_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make some visualizations of the most important features to see how they affect survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First create a dataframe of some features for visualization and concatenate the survival results\n",
    "cols = ['Pclass', 'Age', 'Fare', 'Title', 'Deck', 'Family', 'Sex']\n",
    "vis_df = DataFrame(train_df, columns=cols)\n",
    "vis_df['Survived'] = target\n",
    "sex_map = {0:'Female', 1:'Male'}\n",
    "survival_map = {0:'Perished', 1:'Survived'}\n",
    "class_map = {1:'1st', 2:'2nd', 3:'3rd'}\n",
    "vis_df.Sex = vis_df.Sex.map(sex_map)\n",
    "vis_df.Survived = vis_df.Survived.map(survival_map)\n",
    "vis_df.Pclass = vis_df.Pclass.map(class_map)\n",
    "\n",
    "# Create visualizations\n",
    "sb.countplot(x='Sex', hue='Survived', data=vis_df)\n",
    "plt.title('Female passengers had better chances')\n",
    "plt.show()\n",
    "\n",
    "sb.countplot(x='Pclass', hue='Survived', data=vis_df, order=['1st', '2nd', '3rd'])\n",
    "plt.title('Class and survival played out exactly as expected')\n",
    "plt.show()\n",
    "\n",
    "sb.factorplot(x='Survived', y='Fare', hue='Pclass', data=vis_df, hue_order=['1st', '2nd', '3rd'])\n",
    "plt.title('At least in 1st class, higher fare also increased survival chances')\n",
    "plt.show()\n",
    "\n",
    "sb.factorplot(x='Survived', y='Age', hue='Sex', data=vis_df)\n",
    "plt.title('Age had a large but opposite effect depending on gender')\n",
    "plt.show()\n",
    "\n",
    "sb.countplot(x='Family', hue='Survived', data=vis_df, order=['Single', 'Small', 'Large'])\n",
    "plt.title('A large family seems to have not helped')\n",
    "plt.show()\n",
    "\n",
    "sb.countplot(x='Title', hue='Survived', data=vis_df)\n",
    "plt.title('Aside from gender, royalty seemed to help. Unless most royals were female?')\n",
    "plt.show()\n",
    "\n",
    "sb.countplot(x='Title', hue='Sex', data=vis_df)\n",
    "plt.title('It seems most royals were male - so being royal helped regardless of gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I take the original combined dataframe and drop columns that aren't needed anymore and normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop columns for which dummy columns exist from the combined dataframe\n",
    "combined_df = combined_df.drop(['Title', 'Deck', 'Family', 'Pclass'], axis=1)\n",
    "\n",
    "# Rescale all features to exist within the unit interval\n",
    "feature_list = list(combined_df.columns)\n",
    "feature_list.remove('PassengerId')\n",
    "combined_df[ feature_list ] = combined_df[ feature_list ].apply(lambda x: x/x.max(), axis=0)\n",
    "\n",
    "# Split the combined dataframe into a train and test set again\n",
    "train_df = combined_df.loc[ 0:(train.shape[0]-1) ]\n",
    "test_df = combined_df.loc[ train.shape[0]:(train.shape[0] + test.shape[0]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I perform feature selection which helps to reduce redundancy in the data, speeds up training, and reduces overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform feature selection using a tree-based estimator. The threshold for selection is by default set to the mean\n",
    "# of the importances.\n",
    "clf = ExtraTreesClassifier(n_estimators=200)\n",
    "_ = clf.fit(train_df, target)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "selected_train = model.transform(train_df)\n",
    "selected_test = model.transform(test_df)\n",
    "\n",
    "# Create a dataframe to map features to their importances\n",
    "features = DataFrame()\n",
    "features['feature'] = train_df.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "\n",
    "# Print out important features\n",
    "features[ features.importance > np.mean(features.importance) ].sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I perform a grid search with a 5-fold cross validation to tune the hyper parameters of a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rand_forest = RandomForestClassifier()\n",
    "\n",
    "# Define a parameter grid to search over\n",
    "param_grid = {\"n_estimators\": range(250,360,10), \"max_depth\": range(6,17,2), \n",
    "              'criterion':['gini','entropy']}\n",
    "\n",
    "# Perform 5-fold stratified cross validation\n",
    "cross_validation = StratifiedKFold(target, n_folds=5)\n",
    "\n",
    "# Perform grid search\n",
    "clf = GridSearchCV(rand_forest, param_grid=param_grid, \n",
    "                   cv=cross_validation, n_jobs=10, \n",
    "                   verbose=10)\n",
    "_ = clf.fit(selected_train, target)\n",
    "\n",
    "print 'Best score: %.2f' % clf.best_score_\n",
    "print 'Best parameters: {}'.format(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I fill a dataframe with my survival predictions and export it in csv format for submission. This result achieved an accuracy of 0.79904 on the public leaderboard for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = clf.predict(selected_test).astype(int)\n",
    "result_df = DataFrame(columns=['PassengerId', 'Survived'])\n",
    "result_df.PassengerId = test_df.PassengerId\n",
    "result_df.Survived = result\n",
    "result_df.to_csv('../data/titanic_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
